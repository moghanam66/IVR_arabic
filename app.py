from flask import Flask, request, jsonify, send_from_directory, Response
import os
import asyncio
import openai
import pandas as pd
import ast
from azure.search.documents import SearchClient
from azure.search.documents.models import VectorizedQuery
from azure.core.credentials import AzureKeyCredential
import redis
import azure.cognitiveservices.speech as speechsdk
from rtclient import ResponseCreateMessage, RTLowLevelClient, ResponseCreateParams
import json
import time
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from flask_cors import CORS
 
# Import Bot Framework dependencies
from botbuilder.core import BotFrameworkAdapter, BotFrameworkAdapterSettings, TurnContext
from botbuilder.schema import Activity
 
app = Flask(__name__)
CORS(app)  # Enable CORS for all routes
 
# ------------------------------------------------------------------
# Configuration for Azure OpenAI, GPT‚Äë4o realtime, Azure Search, Redis, Speech
# ------------------------------------------------------------------
 
# Azure OpenAI configuration for embeddings
OPENAI_API_KEY = "8929107a6a6b4f37b293a0fa0584ffc3"
OPENAI_API_VERSION = "2023-03-15-preview"
OPENAI_ENDPOINT = "https://genral-openai.openai.azure.com/"
EMBEDDING_MODEL = "text-embedding-ada-002"  # Fast embedding model
 
# GPT‚Äë4o realtime
RT_API_KEY = "9e76306d48fb4e6684e4094d217695ac"
RT_ENDPOINT = "https://general-openai02.openai.azure.com/"
RT_DEPLOYMENT = "gpt-4o-realtime-preview"
RT_API_VERSION = "2024-10-17"
 
# Azure Cognitive Search
SEARCH_SERVICE_NAME = "mainsearch01"          
SEARCH_INDEX_NAME = "id"                      
SEARCH_API_KEY = "Y6dbb3ljV5z33htXQEMR8ICM8nAHxOpNLwEPwKwKB9AzSeBtGPav"
 
# Redis
REDIS_HOST = "AiKr.redis.cache.windows.net"
REDIS_PORT = 6380
REDIS_PASSWORD = "OD8wyo8NiVxse6DDkEY19481Xr7ZhQAnfAzCaOZKR2U="
 
# Speech
SPEECH_KEY = "3c358ec45fdc4e6daeecb7a30002a9df"
SPEECH_REGION = "westus2"
 
# Thresholds for determining whether a search result is ‚Äúgood enough.‚Äù
SEMANTIC_THRESHOLD = 3.4
VECTOR_THRESHOLD = 0.91
 
# ------------------------------------------------------------------
# Initialize clients and load data
# ------------------------------------------------------------------
 
# Initialize the Azure OpenAI client (for embeddings)
client = openai.AzureOpenAI(
    api_key=OPENAI_API_KEY,
    api_version=OPENAI_API_VERSION,
    azure_endpoint=OPENAI_ENDPOINT
)
 
# Load Q&A data
try:
    qa_data = pd.read_csv("qa_data.csv", encoding="windows-1256")
    print("‚úÖ CSV file loaded successfully!")
    print(qa_data.head())
except Exception as e:
    print(f"‚ùå Failed to load CSV file: {e}")
    exit()
 
# Normalize column names (convert to lowercase, trim spaces)
qa_data.rename(columns=lambda x: x.strip().lower(), inplace=True)
 
# Convert the 'id' column to string (fix type conversion error)
if "id" in qa_data.columns:
    qa_data["id"] = qa_data["id"].astype(str)
 
# Verify required columns exist
if "question" not in qa_data.columns or "answer" not in qa_data.columns:
    print("‚ùå CSV file must contain 'question' and 'answer' columns.")
    exit()
 
# EMBEDDING GENERATION
def get_embedding(text):
    """
    Generate an embedding for the given text using the OpenAI model.
    """
    try:
        response = client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text
        )
        embedding = response.data[0].embedding
        print(f"‚úÖ Embedding generated for text: {text}")
        return embedding
    except Exception as e:
        print(f"‚ùå Failed to generate embedding for text '{text}': {e}")
        return None
 
# Generate embeddings if not already present
if "embedding" not in qa_data.columns or qa_data["embedding"].isnull().all():
    qa_data["embedding"] = qa_data["question"].apply(get_embedding)
    qa_data.to_csv("embedded_qa_data.csv", index=False)
    print("‚úÖ Embeddings generated and saved to 'embedded_qa_data.csv'.")
else:
    def convert_embedding(x):
        if isinstance(x, str):
            try:
                return ast.literal_eval(x)
            except Exception as e:
                print("‚ùå Failed to parse embedding:", e)
                return None
        return x
    qa_data["embedding"] = qa_data["embedding"].apply(convert_embedding)
    print("‚úÖ Using existing embeddings from CSV.")
 
# Normalize question text for consistent matching.
qa_data["question"] = qa_data["question"].str.strip().str.lower()
 
# UPLOAD DOCUMENTS TO AZURE COGNITIVE SEARCH
search_client = SearchClient(
    endpoint=f"https://{SEARCH_SERVICE_NAME}.search.windows.net/",
    index_name=SEARCH_INDEX_NAME,
    credential=AzureKeyCredential(SEARCH_API_KEY)
)
 
documents = qa_data.to_dict(orient="records")
try:
    upload_result = search_client.upload_documents(documents=documents)
    print(f"‚úÖ Uploaded {len(documents)} documents to Azure Search. Upload result: {upload_result}")
except Exception as e:
    print(f"‚ùå Failed to upload documents: {e}")
 
# Debug: Run a simple query to verify that documents are in the index.
try:
    simple_results = search_client.search(
        search_text="*",
        select=["question", "answer"],
        top=3
    )
    print("Simple query results:")
    for doc in simple_results:
        print(doc)
except Exception as e:
    print(f"‚ùå Simple query error: {e}")
 
# INITIALIZE REDIS CLIENT
try:
    redis_client = redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        db=0,
        ssl=True,
        decode_responses=True,
        password=REDIS_PASSWORD
    )
    redis_client.ping()
    print("‚úÖ Successfully connected to Redis!")
except Exception as e:
    print(f"‚ùå Failed to connect to Redis: {e}")
 
# ------------------------------------------------------------------
# SEARCH & RESPONSE FUNCTIONS
# ------------------------------------------------------------------
 
def check_redis_cache(query):
    """Return cached answer if it exists."""
    try:
        cached_answer = redis_client.get(query)
        if cached_answer:
            print(f"‚úÖ Using cached answer for query: {query}")
            return cached_answer
    except Exception as e:
        print(f"‚ùå Redis error: {e}")
    return None
 
def get_best_match(query):
    """
    Retrieve the best answer for the query by trying semantic then vector search.
    """
    cached_response = check_redis_cache(query)
    if cached_response:
        return cached_response
 
    # --- Semantic Search ---
    try:
        semantic_results = search_client.search(
            search_text=query,
            query_type="semantic",
            semantic_configuration_name="my-semantic-config-default",
            query_caption="extractive",
            select=["question", "answer"],
            top=3
        )
        semantic_answer = next(semantic_results, None)
        if semantic_answer:
            reranker_score = semantic_answer["@search.reranker_score"]
            if reranker_score is not None and reranker_score >= SEMANTIC_THRESHOLD:
                answer = semantic_answer["answer"]
                print("‚úÖ Found match using Semantic Search with score", reranker_score)
                redis_client.set(query, answer, ex=3600)
                return answer
            else:
                print("‚ùå Semantic search result score below threshold:", reranker_score)
        else:
            print("‚ùå No semantic search answers found.")
    except Exception as e:
        print(f"‚ùå Semantic search error: {e}")
 
    # --- Vector Search ---
    try:
        query_embedding = client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=query
        ).data[0].embedding
 
        vector_query = VectorizedQuery(
            vector=query_embedding,
            k_nearest_neighbors=50,
            fields="embedding"
        )
        vector_results = search_client.search(
            search_text=None,
            vector_queries=[vector_query],
            select=["question", "answer"],
            top=3
        )
        best_vector = next(vector_results, None)
        if best_vector:
            score = best_vector.get("@search.score", 0)
            if score >= VECTOR_THRESHOLD:
                answer = best_vector["answer"]
                print("‚úÖ Found match using Vector Search with score", score)
                redis_client.set(query, answer, ex=3600)
                return answer
            else:
                print("‚ùå Vector search result score below threshold:", score)
        else:
            print("‚ùå No vector search results found.")
    except Exception as e:
        print(f"‚ùå Vector search error: {e}")
 
    print("‚ùå No match found using Semantic or Vector Search")
    return None
 
# GPT‚Äë4o REALTIME FALLBACK (ASYNC)
async def get_realtime_response(user_query):
    """
    Fallback function: Uses GPT‚Äë4o realtime to generate an answer if both searches fail.
    Now with added instructions so that the model responds as an Egyptian man.
    """
    try:
        async with RTLowLevelClient(
            url=RT_ENDPOINT,
            azure_deployment=RT_DEPLOYMENT,
            key_credential=AzureKeyCredential(RT_API_KEY)
        ) as client_rt:
            # Prepend instruction for Egyptian persona
            instructions = "ÿ£ŸÜÿ™ ÿ±ÿ¨ŸÑ ÿπÿ±ÿ®Ÿä. ÿßŸÜÿß ŸÑÿß ÿßÿ±ŸäÿØ ÿßŸäÿ∂ÿß ÿßŸä bold points  ŸÅŸä ÿßŸÑÿßÿ¨ÿßÿ®ÿ©  Ÿà ŸÑÿß ÿßÿ±ŸäÿØ ÿπŸÜŸàÿßŸäŸÜ ŸÖÿ±ŸÇŸÖÿ©" + user_query
            await client_rt.send(
                ResponseCreateMessage(
                    response=ResponseCreateParams(
                        modalities={"text"},
                        instructions=instructions
                    )
                )
            )
            done = False
            response_text = ""
            while not done:
                message = await client_rt.recv()
                if message is None:
                    print("‚ùå No message received from the real-time service.")
                    break
                if message.type == "response.done":
                    done = True
                elif message.type == "error":
                    done = True
                    print(f"Error: {message.error}")
                elif message.type == "response.text.delta":
                    response_text += message.delta
            return response_text
    except Exception as e:
        print(f"‚ùå Failed to get real-time response: {e}")
        return "ÿπÿ∞ÿ±Ÿãÿßÿå ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ŸÖÿ≠ÿßŸàŸÑÿ© ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ÿÆÿØŸÖÿ© ÿßŸÑÿØÿπŸÖ ÿßŸÑŸÅŸàÿ±Ÿä. Ÿäÿ±ÿ¨Ÿâ ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ ŸÑÿßÿ≠ŸÇŸãÿß."
 
async def get_response(user_query):
    """
    Retrieve a response by first trying search (semantic then vector),
    then falling back to GPT‚Äë4o realtime if no match is found.
    """
    print(f"üîç Processing query: {user_query}")
    response = get_best_match(user_query)
    if response:
        print(f"‚úÖ Found response in cache or search: {response}")
        return response
 
    print("üîç No match found, falling back to GPT‚Äë4o realtime...")
    realtime_response = await get_realtime_response(user_query)
    if realtime_response:
        print(f"‚úÖ GPT‚Äë4o realtime response: {realtime_response}")
        try:
            redis_client.set(user_query, realtime_response, ex=3600)
            print("‚úÖ Response cached in Redis.")
        except Exception as e:
            print(f"‚ùå Failed to cache response in Redis: {e}")
        return realtime_response
    else:
        return "ÿπÿ∞ÿ±Ÿãÿßÿå ŸÑŸÖ ÿ£ÿ™ŸÖŸÉŸÜ ŸÖŸÜ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿ•ÿ¨ÿßÿ®ÿ©. Ÿäÿ±ÿ¨Ÿâ ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ ŸÑÿßÿ≠ŸÇŸãÿß."
 
# ------------------------------------------------------------------
# SPEECH RECOGNITION & SYNTHESIS SETUP
# ------------------------------------------------------------------
 
speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)
speech_config.speech_recognition_language = "ar-EG"
speech_config.speech_synthesis_voice_name = "ar-EG-ShakirNeural"
 
def recognize_speech():
    """Listen for a single utterance using the default microphone."""
    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)
    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
    print("Listening... (Speak in Egyptian Arabic)")
    result = recognizer.recognize_once_async().get()
    if result.reason == speechsdk.ResultReason.RecognizedSpeech:
        print(f"You said: {result.text}")
        return result.text
    else:
        print(f"Speech not recognized: {result.reason}")
        return ""
 
def speak_response(text):
    """Convert the given text to speech and output via the default speaker."""
    audio_output = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)
    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_output)
    result = synthesizer.speak_text_async(text).get()
    if result.reason == speechsdk.ResultReason.Canceled:
        cancellation = result.cancellation_details
        print("Speech synthesis canceled:")
        print("  Reason: {}".format(cancellation.reason))
        print("  Error Details: {}".format(cancellation.error_details))
 
# HELPER: CLEAN TEXT FOR EXIT CHECK
def clean_text(text):
    """
    Remove common punctuation and whitespace from the beginning and end of the text,
    then convert to lower case.
    """
    return text.strip(" .ÿå!ÿõÿü").lower()
 
# CRITICAL ISSUE DETECTION
def detect_critical_issue(text):
    """
    Detect if the user's input contains a critical issue that should be passed to a human.
    """
    # Arabic Trigger Sentences
    trigger_sentences = [
        "ÿ™ŸÖ ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßÿÆÿ™ÿ±ÿßŸÇ ÿ£ŸÖŸÜŸä ŸÉÿ®Ÿäÿ±.",
        "ÿ™ŸÖŸÉŸÜ ŸÇÿ±ÿßÿµŸÜÿ© ŸÖŸÜ ÿßŸÑŸàÿµŸàŸÑ ÿ•ŸÑŸâ ÿ®ŸäÿßŸÜÿßÿ™ ÿ≠ÿ≥ÿßÿ≥ÿ©.",
        "ŸáŸÜÿßŸÉ Ÿáÿ¨ŸàŸÖ ÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä ÿπŸÑŸâ ÿßŸÑŸÜÿ∏ÿßŸÖ ÿßŸÑÿÆÿßÿµ ÿ®ŸÜÿß.",
        "ÿ™ŸÖ ÿ™ÿ≥ÿ±Ÿäÿ® ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖŸäŸÜ ÿ•ŸÑŸâ ÿßŸÑÿ•ŸÜÿ™ÿ±ŸÜÿ™.",
        "ÿ±ÿµÿØŸÜÿß ŸÖÿ≠ÿßŸàŸÑÿ© ÿ™ÿµŸäÿØ ÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä ÿ∂ÿØ ŸÖŸàÿ∏ŸÅŸäŸÜÿß.",
        "ÿ™ŸÖ ÿßÿ≥ÿ™ÿ∫ŸÑÿßŸÑ ÿ´ÿ∫ÿ±ÿ© ÿ£ŸÖŸÜŸäÿ© ŸÅŸä ÿßŸÑÿ¥ÿ®ŸÉÿ©.",
        "ŸáŸÜÿßŸÉ ŸÖÿ≠ÿßŸàŸÑÿ© ŸàÿµŸàŸÑ ÿ∫Ÿäÿ± ŸÖÿµÿ±ÿ≠ ÿ®Ÿáÿß ÿ•ŸÑŸâ ÿßŸÑŸÖŸÑŸÅÿßÿ™ ÿßŸÑÿ≥ÿ±Ÿäÿ©."
    ]
 
    # Get embeddings for trigger sentences
    trigger_embeddings = np.array([get_embedding(sent) for sent in trigger_sentences])
 
    # Get embedding for the input text
    text_embedding = np.array(get_embedding(text)).reshape(1, -1)
 
    # Calculate cosine similarity between the input text and trigger sentences
    similarities = cosine_similarity(text_embedding, trigger_embeddings)
    max_similarity = np.max(similarities)
 
    # If the similarity is above a threshold, consider it a critical issue
    if max_similarity > 0.9:
        print("This issue should be passed to a human.")
        return True
    return False
 
# ------------------------------------------------------------------
# ASYNCHRONOUS VOICE CHAT LOOP & ROUTES
# ------------------------------------------------------------------
 
async def voice_chat_loop():
    print("ü§ñ Arabic Voice Bot Ready! Say 'ÿ•ŸÜŸáÿßÿ°' or 'ÿÆÿ±Ÿàÿ¨' to exit.")
    while True:
        user_query = recognize_speech()
        if clean_text(user_query) in ["ÿ•ŸÜŸáÿßÿ°", "ÿÆÿ±Ÿàÿ¨"]:
            print("üëã Goodbye!")
            speak_response("ŸÖÿπ ÿßŸÑÿ≥ŸÑÿßŸÖÿ©!")
            break
 
        if detect_critical_issue(user_query):
            response = "Ÿáÿ∞Ÿá ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© ÿ™ÿ≠ÿ™ÿßÿ¨ ÿ•ŸÑŸâ ÿ™ÿØÿÆŸÑ ÿ®ÿ¥ÿ±Ÿä. ÿ≥ÿ£ŸÇŸàŸÖ ÿ®ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ÿÆÿØŸÖÿ© ÿßŸÑÿπŸÖŸÑÿßÿ° ŸÑÿØÿπŸÖŸÉ."
            print(f"ü§ñ Bot: {response}")
            speak_response(response)
            continue
 
        response = await get_response(user_query)
        print(f"ü§ñ Bot: {response}")
        speak_response(response)
 
async def voice_chat(user_query):
    try:
        # Accept text input from the client
        if not user_query:
            return "ŸÅŸä ÿßŸÜÿ™ÿ∏ÿßÿ± ÿßŸàÿßŸÖÿ±ŸÉ"
        if clean_text(user_query) in ["ÿ•ŸÜŸáÿßÿ°", "ÿÆÿ±Ÿàÿ¨"]:
            print("üëã Goodbye!")
            return "ŸÖÿπ ÿßŸÑÿ≥ŸÑÿßŸÖÿ©"
        if detect_critical_issue(user_query):
            response = "Ÿáÿ∞Ÿá ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© ÿ™ÿ≠ÿ™ÿßÿ¨ ÿ•ŸÑŸâ ÿ™ÿØÿÆŸÑ ÿ®ÿ¥ÿ±Ÿä. ÿ≥ÿ£ŸÇŸàŸÖ ÿ®ÿßŸÑÿßÿ™ÿµÿßŸÑ ÿ®ÿÆÿØŸÖÿ© ÿßŸÑÿπŸÖŸÑÿßÿ° ŸÑÿØÿπŸÖŸÉ."
            return response
        # Process the query and generate a response
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        response = loop.run_until_complete(get_response(user_query))
        loop.close()
 
        return response
    except Exception as e:
        print(f"Error in /voice-chat: {e}")
 
@app.route("/")
def index():
    return send_from_directory("static", "index.html")
 
# ------------------------------------------------------------------
# Bot Framework Integration
# ------------------------------------------------------------------
 
# Bot Framework credentials (set via environment or hard-code for testing)
MICROSOFT_APP_ID = "b0a29017-ea3f-4697-aef7-0cb05979d16c"
MICROSOFT_APP_PASSWORD = "2fc8Q~YUZMbD8E7hEb4.vQoDFortq3Tvt~CLCcEQ"
 
# Initialize Bot Framework adapter
adapter_settings = BotFrameworkAdapterSettings(MICROSOFT_APP_ID, MICROSOFT_APP_PASSWORD)
adapter = BotFrameworkAdapter(adapter_settings)
 
# Define a bot class that uses your get_response logic
class VoiceChatBot:
    async def on_turn(self, turn_context: TurnContext):
        if turn_context.activity.type == "message":
            user_query = turn_context.activity.text
            print(f"Received message via Bot Framework: {user_query}")
            response_text = await voice_chat(user_query)
            await turn_context.send_activity(response_text)
        else:
            await turn_context.send_activity(f"Received activity of type: {turn_context.activity.type}")
 
# Create an instance of the bot
bot = VoiceChatBot()
 
# Bot Framework messaging endpoint
@app.route("/api/messages", methods=["POST"])
def messages():
    if request.headers.get("Content-Type", "") != "application/json":
        return Response(status=415)
    body = request.json
    activity = Activity().deserialize(body)
    # Use a dummy token if none is provided (only for local testing)
    auth_header = request.headers.get("Authorization", "")
    if not auth_header:
        auth_header = "eyJ0eXAiOiJKV1QiLCJub25jZSI6Im5XV2RDRXBEVDlfWUNUNDdZeVdlSlpGZG83eGxzdVRWMi0wcl9EeUdkQ00iLCJhbGciOiJSUzI1NiIsIng1dCI6ImltaTBZMnowZFlLeEJ0dEFxS19UdDVoWUJUayIsImtpZCI6ImltaTBZMnowZFlLeEJ0dEFxS19UdDVoWUJUayJ9.eyJhdWQiOiJodHRwczovL2dyYXBoLm1pY3Jvc29mdC5jb20vIiwiaXNzIjoiaHR0cHM6Ly9zdHMud2luZG93cy5uZXQvMTM4ZTJmYmEtODBkYS00OTEzLTk1MjQtNzg4NDIzMjRlMDc4LyIsImlhdCI6MTc0MDQwMDc2NCwibmJmIjoxNzQwNDAwNzY0LCJleHAiOjE3NDA0MDYyMjEsImFjY3QiOjEsImFjciI6IjEiLCJhaW8iOiJBWlFBYS84WkFBQUFqSzZldTRJQ25oQ3NDaUd6T3h0bnRFQkRoODdXaXdNWE5mdjBTSlVCV1c2dUpOV1pJVmVsc2VIaTZ4VHR6Y2FIK21vL1gzVng3ajFodnNsMFJxOE9hQUczOVFsSzErZlFpYlhtSlNCbzdWeU1MNWtFa1ZRZmpQS3MrV2NGUnJvQldlQ1FmenN5WWU0L0hhcUpPM3FjZ1lpNE1qandZWW9kNk9wdjJnL0crcE9KaytHd1MvZ1oxSFU4N3kvR0xKTEoiLCJhbHRzZWNpZCI6IjU6OjEwMDMyMDA0MEZCNjdGOTAiLCJhbXIiOlsicHdkIl0sImFwcF9kaXNwbGF5bmFtZSI6Iml2cl9hcmFiaWNfbGluayIsImFwcGlkIjoiYzU0ODQ0MDItYWY3Yy00ZTViLTg3NjYtZDk5NjY5MDE3YmM0IiwiYXBwaWRhY3IiOiIxIiwiZW1haWwiOiJtb2hhbWVkLmdoYW5hbUBsaW5rZGV2LmNvbSIsImlkcCI6Imh0dHBzOi8vc3RzLndpbmRvd3MubmV0L2IwODQwMzFiLWNkYjEtNDIyZS1hNzZhLWE2NTEwY2RiMTdmOC8iLCJpZHR5cCI6InVzZXIiLCJpcGFkZHIiOiIxOTcuNTQuMjYuMjIwIiwibmFtZSI6Ik1vaGFtZWQgR2hhbmFtIiwib2lkIjoiZTNhN2M4ODctNTk1Yy00ZDYyLTk1OTItNzdmYTY2NmEwNzA2IiwicGxhdGYiOiIzIiwicHVpZCI6IjEwMDMyMDA0NTNEQTYyRTYiLCJyaCI6IjEuQVY0QXVpLU9FOXFBRTBtVkpIaUVJeVRnZUFNQUFBQUFBQUFBd0FBQUFBQUFBQUFSQVhwZUFBLiIsInNjcCI6IlVzZXIuUmVhZCIsInNpZCI6IjMxOTdlYzkzLTk2NDgtNGZiOS1iNzM1LWY3YTFiODEwNDczYSIsInNpZ25pbl9zdGF0ZSI6WyJrbXNpIl0sInN1YiI6IjZLOTRsUXBObGkzU1g4ZERsZWNySFR1RDJHc3BFSFEwQXlYY2xWZFFoZ2ciLCJ0ZW5hbnRfcmVnaW9uX3Njb3BlIjoiRVUiLCJ0aWQiOiIxMzhlMmZiYS04MGRhLTQ5MTMtOTUyNC03ODg0MjMyNGUwNzgiLCJ1bmlxdWVfbmFtZSI6Im1vaGFtZWQuZ2hhbmFtQGxpbmtkZXYuY29tIiwidXRpIjoicU5JRzdWREoxME9ydVg1b05XQ2FBQSIsInZlciI6IjEuMCIsIndpZHMiOlsiY2YxYzM4ZTUtMzYyMS00MDA0LWE3Y2ItODc5NjI0ZGNlZDdjIiwiMTNiZDFjNzItNmY0YS00ZGNmLTk4NWYtMThkM2I4MGYyMDhhIl0sInhtc19pZHJlbCI6IjQgNSIsInhtc190Y2R0IjoxNTMyNjEyMjE2fQ.kRTPVdwqttCtOlFdjURBYkjQuhqPVrRf5k_zzRi7Sg1N2C-sbasq4MYf_FK9ddVKRWN_a-vVXyQVvnbbE94yl8IjrlTqTy9Klz4BZOasnQlKQRnNEdVKvrk5z341Jah0aV3dSrlcSSdr46NIwcuHeiwy1TrDm5ZMQX4uUnm91WJN0oIdojc2Q53ZD2l5fVvv63bBlrc8Vs2FfTceMX7BWsTrIAeyyJhK9lwDMQ1ti2esFp7CJSR4Lr8MhCWJK9nRYg3KPqFsClofLg0w3TMHRISL7TkwCFNg8G8RA8lURHMjC-6PNim12fmkwrjVOtHx7WOcWLT6Mgyq2g3KdXqU7Q"
    async def call_bot():
        await adapter.process_activity(activity, auth_header, bot.on_turn)
 
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    task = loop.create_task(call_bot())
    loop.run_until_complete(task)
    return Response(status=201)
 
# ------------------------------------------------------------------
# Run the Flask application
# ------------------------------------------------------------------
 
if __name__ == "__main__":
    app.run(debug=True)
